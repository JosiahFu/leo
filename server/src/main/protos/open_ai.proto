syntax = "proto3";

option java_multiple_files = true;
option java_package = "org.davincischools.leo.protos.open_ai";

package open_ai;

service OpenAiService {
  rpc CreateCompletion(CreateCompletionRequest)
    returns (CreateCompletionResponse) {}
}

// Messages to send / receive.
message CreateCompletionMessage {
  // Role to play. "system" is for the OpenAI request. "assistant" is for the
  // response.
  optional string role = 1;

  // Content of the message.
  optional string content = 2;
}

// See: https://platform.openai.com/docs/api-reference/completions/create
message CreateCompletionRequest {
  // ID of the model to use. For an overview of available models,
  // see: https://platform.openai.com/docs/models/overview
  optional string model = 1;

  // The prompt(s) to generate completions for, encoded as a string, array of
  // strings, array of tokens, or array of token arrays.
  //
  // Note that <|endoftext|> is the document separator that the model sees
  // during training, so if a prompt is not specified the model will generate
  // as if from the beginning of a new document.
  repeated string prompt = 2;  // default = "<|endoftext|>".

  // The suffix that comes after a completion of inserted text.
  optional string suffix = 3;

  // The maximum number of tokens to generate in the completion.
  //
  // The token count of your prompt plus max_tokens cannot exceed the model's
  // context length. Most models have a context length of 2048 tokens (except
  // for the newest models, which support 4096).
  optional int32 max_tokens = 4;  // default = 16.

  // What sampling temperature to use, between 0 and 2. Higher values like 0.8
  // will make the output more random, while lower values like 0.2 will make it
  // more focused and deterministic.
  //
  // We generally recommend altering this or top_p but not both.
  optional float temperature = 5;  // default = 1.

  // An alternative to sampling with temperature, called nucleus sampling,
  // where the model considers the results of the tokens with top_p probability
  // mass. So 0.1 means only the tokens comprising the top 10% probability mass
  // are considered.
  //
  // We generally recommend altering this or temperature but not both.
  optional float top_p = 6;  // default = 1.

  // How many completions to generate for each prompt. Defaults to 1.
  //
  // Note: Because this parameter generates many completions, it can quickly
  // consume your token quota. Use carefully and ensure that you have
  // reasonable settings for max_tokens and stop.
  optional int32 n = 7;  // default = 1.

  // Whether to stream back partial progress. If set, tokens will be sent as
  // data-only server-sent events as they become available, with the stream
  // terminated by a data: [DONE] message.
  optional bool stream = 8;  // default = false.

  // Include the log probabilities on the logprobs most likely tokens, as well
  // the chosen tokens. For example, if logprobs is 5, the API will return a
  // list of the 5 most likely tokens. The API will always return the logprob
  // of the sampled token, so there may be up to logprobs+1 elements in the
  // response.
  //
  // The maximum value for logprobs is 5. If you need more than this, please
  // contact us through our Help center and describe your use case.
  optional int32 logprobs = 9;  // defaults = null.

  // Echo back the prompt in addition to the completion.
  optional bool echo = 10;  // default = false.

  // Up to 4 sequences where the API will stop generating further tokens. The
  // returned text will not contain the stop sequence.
  repeated string stop = 11;  // defaults = null.

  // Number between -2.0 and 2.0. Positive values penalize new tokens based on
  // whether they appear in the text so far, increasing the model's likelihood
  // to talk about new topics.
  //
  // See: https://platform.openai.com/docs/api-reference/parameter-details
  repeated float presence_penalty = 12;  // default = 0.

  // Number between -2.0 and 2.0. Positive values penalize new tokens based on
  // their existing frequency in the text so far, decreasing the model's
  // likelihood to repeat the same line verbatim.
  //
  // See: https://platform.openai.com/docs/api-reference/parameter-details
  optional float frequenty_penalty = 13;  // default = 0.

  // Generates best_of completions server-side and returns the "best" (the one
  // with the highest log probability per token). Results cannot be streamed.
  //
  // When used with n, best_of controls the number of candidate completions and
  // n specifies how many to return â€“ best_of must be greater than n.
  //
  // Note: Because this parameter generates many completions, it can quickly
  // consume your token quota. Use carefully and ensure that you have
  // reasonable settings for max_tokens and stop.
  optional int32 best_of = 14;  // default = 1.

  // Modify the likelihood of specified tokens appearing in the completion.
  //
  // Accepts a json object that maps tokens (specified by their token ID in the
  // GPT tokenizer) to an associated bias value from -100 to 100. You can use
  // this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text
  // to token IDs. Mathematically, the bias is added to the logits generated by
  // the model prior to sampling. The exact effect will vary per model, but
  // values between -1 and 1 should decrease or increase likelihood of
  // selection; values like -100 or 100 should result in a ban or exclusive
  // selection of the relevant token.
  //
  // As an example, you can pass {"50256": -100} to prevent the <|endoftext|>
  // token from being generated.
  map<string, float> logit_bias = 15;  // default = null.

  // A unique identifier representing your end-user, which can help OpenAI to
  // monitor and detect abuse.
  optional string user = 16;

  repeated CreateCompletionMessage messages = 17;
}

// Represents a generated completion response.
message CreateCompletionResponse {
  // A unique identifier for the completion generated by the API request
  string id = 1;

  // The type of object returned by the API, which is typically
  // "chat.completion".
  optional string object = 2;

  // The generated text completion.
  optional string text = 3;

  // The timestamp of when the completion was created.
  optional int64 created = 4;

  // The ID of the GPT model used to generate the completion.
  optional string model = 5;

  // Each completion object typically includes:
  message CreateCompletionChoice {
    // The generated text completion.
    optional string text = 1;
    // The index of the completion in the list of n possible completions.
    optional int32 index = 2;
    // An array of log probabilities for each token in the generated completion.
    repeated float logprobs = 3;
    // The quality of the generated completion, between 0 and 1 (only populated
    // if logprobs = true)
    float prompt_completion_score = 4;
    // A string indicating why the completion was terminated, if applicable.
    optional string finish_reason = 5;
    // The message of the choice.
    optional CreateCompletionMessage message = 6;
  }
  // An array of possible completions (only populated if n > 1)
  // A list of completion objects, where each object represents a potential
  // continuation of the input prompt.
  repeated CreateCompletionChoice choices = 6;

  // An array of probabilities for each choice (only populated if n > 1)
  repeated float probabilities = 7;

  // Whether the API has finished generating the completion
  bool finished = 8;

  // The text prompt used to generate the completion.
  optional string prompt = 9;
}

